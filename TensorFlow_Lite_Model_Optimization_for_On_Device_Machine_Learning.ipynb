{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Senbonsakurakageyoshi1/Senbonsakurakageyoshi1/blob/main/TensorFlow_Lite_Model_Optimization_for_On_Device_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh3ONjhNH8fZ"
      },
      "source": [
        "# TensorFlow Lite: Model Optimization for On-Device Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZapxxDtL-Lz"
      },
      "source": [
        "## Base Model Training\n",
        "Import the necessary libraries and packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgnlFTMnIfR0"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "We can directly import the dataset from the **TensorFlow Dataset (tfds)**. We will split the dataset into training, validation and testing set with a split ratio of 0.7:0.2:0.1. The as_supervised parameter is kept True as we need the labels of the images for classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbY-KGMPvbW9"
      },
      "outputs": [],
      "source": [
        "# Load Cat vs Dog dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs', split=['train[:70%]', 'train[70%:90%]', 'train[90%:]'], shuffle_files=True, as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dLImYtWIm8u"
      },
      "source": [
        "Let us now have a look at the dataset information provided in ***tfds.info()***. The dataset has two classes labelled as ‘cat’ and ‘dog’ with 16283, 4653, 2326 training, validation and testing images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAN9xXvkBpE8"
      },
      "outputs": [],
      "source": [
        "print(\"Number of  Classes: \" + str(info.features['label'].num_classes))\n",
        "print(\"Classes : \" + str(info.features['label'].names))\n",
        "\n",
        "NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "\n",
        "print(\"Training Images: \" + str(NUM_TRAIN_IMAGES))\n",
        "\n",
        "NUM_VAL_IMAGES = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "\n",
        "print(\"Validation Images: \" + str(NUM_VAL_IMAGES))\n",
        "\n",
        "NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "\n",
        "print(\"Testing Images: \" + str(NUM_TEST_IMAGES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8A6w6ZGI0xt"
      },
      "source": [
        "Let us now have a look at a few images and their corresponding labels in the training dataset.  The ***tfds.visualization.show_examples()*** function enables us to do so in a single line of code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cssbGKwIvTI"
      },
      "outputs": [],
      "source": [
        "vis = tfds.visualization.show_examples(train_ds, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkj6AmIfI8Jz"
      },
      "source": [
        "We have chosen 16 as batch size and 224x224 as image size so that the dataset can be processed effectively and efficiently. We wil then resize the images in the dataset and use buffered prefetching to yield data from the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm22cwrjJhkV"
      },
      "outputs": [],
      "source": [
        "# Defining Batch Size and input image size.\n",
        "batch_size = 16\n",
        "img_size = [224, 224]\n",
        "\n",
        "# Resizing images in dataset.\n",
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, img_size), y))\n",
        "val_ds = val_ds.map(lambda x, y: (tf.image.resize(x, img_size), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, img_size), y))\n",
        "\n",
        "# Buffering the dataset.\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkvGbf4RKFoY"
      },
      "source": [
        "in order to feed images to the TF Lite model, we need to extract the test images and their labels. We will store them into variables and feed them to TF lIte Model for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P6TnvhrRyWt"
      },
      "outputs": [],
      "source": [
        "# Extracting and saving test images and labels from test dataset.\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for image, label in test_ds.take(len(test_ds)).unbatch():\n",
        "  test_images.append(image)\n",
        "  test_labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pN8OiMoKRgE"
      },
      "source": [
        "We have chosen the EfiicientNet B0 model pre-trained on the imagenet dataset for image classification purposes. EfficientNets are state-of-the-art image classification models. EfficientNets significantly outperform other ConvNets. \n",
        "\n",
        "Let’s import the model now. We have set the input image size to be 224x224 pixels and kept the pooling layer to be GlobalMaxPooling2D. We have unfreezed all the layers of the model so that they can be trainable. We have add Dense layers to the pre-trained model. And also added Dropout and BatchNormalization to reduce overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_curYQ5AKbG"
      },
      "outputs": [],
      "source": [
        "# Defining the model architecture.\n",
        "resnet = tf.keras.applications.EfficientNetB0(include_top = False, weights ='imagenet', input_shape = (224, 224, 3), pooling = 'max')\n",
        "\n",
        "# Unfreezing all the layers of the model.\n",
        "for layer in resnet.layers:\n",
        "  set_trainable = True\n",
        "\n",
        "# Adding Dense, BatchNormalization, and Dropout layers to the base model.\n",
        "x = Dense(512, activation='relu')(resnet.output)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Define the input and output layers of the model.\n",
        "model = Model(inputs=resnet.input, outputs=predictions)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = [\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3i53_WPLW-T"
      },
      "source": [
        "We are using Model Saving Callback and the Reduce LR Callback.\n",
        "\n",
        "\n",
        "1.   Model Saving Callback saves model with best validation accuracy\n",
        "2.   Reduce LR Callback reduces the learning rate by a factor of 0.1 if validation loss remains the same for 3 consecutive epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnIcMwBpumu7"
      },
      "outputs": [],
      "source": [
        "# Defining file path.\n",
        "filepath = '/content/EfnetB0/model.h5'\n",
        "\n",
        "# Defining model Save Callback and Reduce Learning Rate Calllback for achieving better results.\n",
        "model_save = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode=\"max\",\n",
        "    save_freq=\"epoch\")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
        "                                                 factor=0.1, \n",
        "                                                 patience=3, \n",
        "                                                 verbose=1, \n",
        "                                                 min_delta=5*1e-3,\n",
        "                                                 min_lr =5*1e-9,)\n",
        "\n",
        "callback = [reduce_lr, model_save]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf-o9B3YLyaW"
      },
      "source": [
        "We will now train the model using the ***model.fit()*** method. We will pass the training dataset and validation dataset and train the model for 15 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBJXtsSJAMNw"
      },
      "outputs": [],
      "source": [
        "# Training the model for 15 epochs.\n",
        "model.fit(train_ds, epochs=15, steps_per_epoch=(len(train_ds)//batch_size), validation_data=val_ds, validation_steps=(len(val_ds)//batch_size), shuffle=False, callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt0WVpJhNYqM"
      },
      "source": [
        "Let’s check the model’s performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyIKnbVZafIH"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Model on test dataset.\n",
        "_, baseline_model_accuracy = model.evaluate(test_ds, verbose=0)\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo4JuX3Chvko"
      },
      "outputs": [],
      "source": [
        "model.save('/content/EfnetB0/efnetb0_saved_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_bUZBLs_q4N"
      },
      "outputs": [],
      "source": [
        "# Function for evaluating TFLite model over test images.\n",
        "def evaluate(interpreter):\n",
        "  prediction= []\n",
        "\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "  input_format = interpreter.get_output_details()[0]['dtype']\n",
        "  \n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 100 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_format)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.tensor(output_index)\n",
        "    predicted_label = np.argmax(output()[0])\n",
        "    prediction.append(predicted_label)\n",
        "    \n",
        "  print('\\n')\n",
        "  \n",
        "  # Comparing prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction = np.array(prediction)\n",
        "  accuracy = (prediction == test_labels).mean()\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQxVYXC6TKPu"
      },
      "source": [
        "## Quanttization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaC_lnoETO6S"
      },
      "source": [
        "Quantization works by reducing the precision of the numbers used to represent a model's parameters, which by default are 32-bit floating-point numbers. This results in a smaller model size and faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihsxzth3dGh7"
      },
      "source": [
        "### Float 16 Quantaziation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1IrQustTRpL"
      },
      "source": [
        "In Float-16 quantization, weights are converted to 16-bit floating-point values. This results in a 2x reduction in model size. There is a significant reduction in model size in exchange for minimal impacts to latency and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg7E3Aa-dMZT"
      },
      "outputs": [],
      "source": [
        "# Passing Keras Model to TF Lite Converter.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Using float 16 quantization.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Converting the model \n",
        "tflite_fp16_model = converter.convert()\n",
        "\n",
        "# Saving the model.\n",
        "with open('/content/EfnetB0/fp_16_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_fp16_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isci-G-4TZFL"
      },
      "source": [
        "We have passed the Float 16 quantization to the ***converter.target_spec.supported_type*** to specify the type of quantization. The rest of the code remains the same for a general way of conversion for the TF Lite Model.\n",
        "\n",
        "\n",
        "Let’s check this Float 16 quantized TF Lite’s model performance on the Test Set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3-KHpjyJwV8"
      },
      "outputs": [],
      "source": [
        "# Passing FP16 TFLite model to the interpreter\n",
        "interpreter = tf.lite.Interpreter('/content/EfnetB0/fp_16_model.tflite')\n",
        "# Allocating tensors.\n",
        "interpreter.allocate_tensors()\n",
        "# Evaluating the model on test dataset.\n",
        "test_accuracy = evaluate(interpreter)\n",
        "print('Float 16 Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
        "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjWEh1RPemp0"
      },
      "source": [
        "### Integer Qunatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJAEbffvTlVV"
      },
      "source": [
        "Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers. This resulted in a smaller model and increased inferencing speed.\n",
        "\n",
        "The integer quantization requires a representative dataset, i.e. a few images from the training dataset, for the conversion to happen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7dV2wHKGfxp"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/EfnetB0/model.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HZ8cy4gebR_"
      },
      "outputs": [],
      "source": [
        "# Passing the baseline Keras model to TFLite converter.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Defining the representative dataset from training images.\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(test_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Using integer quantization.\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Setting the input and output tensors to uint8 (APIs added in r2.3).\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Converting the model.\n",
        "int_quant_model = converter.convert()\n",
        "\n",
        "# Saving the integer quantized TFLite model.\n",
        "with open('/content/EfnetB0/int_quant_model.tflite', 'wb') as f:\n",
        "  f.write(int_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "512WLD_NT0We"
      },
      "source": [
        "Let’s evaluate the obtained Integer Quantized TF Lite model on Test Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51oTK0kYKxZw"
      },
      "outputs": [],
      "source": [
        "# Passing the integer quantized TFLitemodel to the interpreter.\n",
        "interpreter = tf.lite.Interpreter('/content/EfnetB0/int_quant_model.tflite')\n",
        "# Allocating tensors.\n",
        "interpreter.allocate_tensors()\n",
        "# Evaluating the model on test images.\n",
        "test_accuracy = evaluate(interpreter)\n",
        "print('Integer Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
        "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCVCrMr6eVHc"
      },
      "source": [
        "### Dynamic Range Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2h0klKdVKVk"
      },
      "source": [
        "In Dynamic Range Quantization, weights are converted to 8-bit precision values. Dynamic range quantization achieves a 4x reduction in the model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP7gag-fdR--"
      },
      "outputs": [],
      "source": [
        "# Passing baseline Keras model to TFLite converter.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# Using Dynamic Range quantization.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Converting the model.\n",
        "tflite_quant_model = converter.convert()\n",
        "# Saving the model.\n",
        "with open('/content/EfnetB0/dynamic_quant_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nn_k_NxVOav"
      },
      "source": [
        "Let’s evaluate this TF Lite model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UnMpwhqLmqrF"
      },
      "outputs": [],
      "source": [
        "# Passing Dynamic Range quantized TFLite model to the interpreter.\n",
        "interpreter = tf.lite.Interpreter('/content/EfnetB0/dynamic_quant_model.tflite') \n",
        "# Allocating tensors.\n",
        "interpreter.allocate_tensors()\n",
        "# Evaluating the model on the test images.\n",
        "test_accuracy = evaluate(interpreter)\n",
        "print('Dynamically  Quantized TFLite Model Test Accuracy:', test_accuracy*100)\n",
        "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": " TensorFlow Lite: Model Optimization for On-Device Machine Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}